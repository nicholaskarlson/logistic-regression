{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details...",
      "author": [],
      "contents": "\r\nThis information on logistic regression is under Creative Commons, i.e.\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\r\nThe information was derived from the Creative Commons document available at\r\nhttps://daviddalpiaz.github.io/appliedstats/logistic-regression.html\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-25T22:41:39-07:00"
    },
    {
      "path": "index.html",
      "title": "Logistic Regression",
      "description": "This webpage has information on logistic regression.\n",
      "author": [],
      "contents": "\r\nLogistic Regression\r\nNote to current readers: This chapter is slightly less tested than previous chapters. Please do not hesitate to report any errors, or suggest sections that need better explanation! Also, as a result, this material is more likely to receive edits.\r\nAfter reading this chapter you will be able to:\r\nUnderstand how generalized linear models are a generalization of ordinary linear models.\r\nUse logistic regression to model a binary response.\r\nApply concepts learned for ordinary linear models to logistic regression.\r\nUse logistic regression to perform classification.\r\nSo far we have only considered models for numeric response variables. What about response variables that only take integer values? What about a response variable that is categorical? Can we use linear models in these situations? Yes! The model that we have been using, which we will call ordinary linear regression, is actually a specific case of the more general, generalized linear model. (Aren’t statisticians great at naming things?)\r\nGeneralized Linear Models\r\nSo far, we’ve had response variables that, conditioned on the predictors, were modeled using a normal distribution with a mean that is some linear combination of the predictors. This linear combination is what made a linear model “linear.”\r\n\\[\r\nY \\mid {\\bf X} = {\\bf x} \\sim N(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_{p - 1}x_{p - 1}, \\ \\sigma^2)\r\n\\]\r\nNow we’ll allow for two modifications of this situation, which will let us use linear models in many more situations. Instead of using a normal distribution for the response conditioned on the predictors, we’ll allow for other distributions. Also, instead of the conditional mean being a linear combination of the predictors, it can be some function of a linear combination of the predictors.\r\nIn general, a generalized linear model has three parts:\r\nA distribution of the response conditioned on the predictors. (Technically this distribution needs to be from the exponential family of distributions.)\r\nA linear combination of the \\(p - 1\\) predictors, \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_{p - 1} x_{p - 1}\\), which we write as \\(\\eta({\\bf x})\\). That is,\r\n\\[\\eta({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots  + \\beta_{p - 1} x_{p - 1}\\]\r\nA link function, \\(g()\\), that defines how \\(\\eta({\\bf x})\\), the linear combination of the predictors, is related to the mean of the response conditioned on the predictors, \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\).\r\n\\[\r\n\\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right).\r\n\\]\r\nThe following table summarizes three examples of a generalized linear model:\r\n\r\nLinear Regression\r\nPoisson Regression\r\nLogistic Regression\r\n\\(Y \\mid {\\bf X} = {\\bf x}\\)\r\n\\(N(\\mu({\\bf x}), \\sigma^2)\\)\r\n\\(\\text{Pois}(\\lambda({\\bf x}))\\)\r\n\\(\\text{Bern}(p({\\bf x}))\\)\r\nDistribution Name\r\nNormal\r\nPoisson\r\nBernoulli (Binomial)\r\n\\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\)\r\n\\(\\mu({\\bf x})\\)\r\n\\(\\lambda({\\bf x})\\)\r\n\\(p({\\bf x})\\)\r\nSupport\r\nReal: \\((-\\infty, \\infty)\\)\r\nInteger: \\(0, 1, 2, \\ldots\\)\r\nInteger: \\(0, 1\\)\r\nUsage\r\nNumeric Data\r\nCount (Integer) Data\r\nBinary (Class ) Data\r\nLink Name\r\nIdentity\r\nLog\r\nLogit\r\nLink Function\r\n\\(\\eta({\\bf x}) = \\mu({\\bf x})\\)\r\n\\(\\eta({\\bf x}) = \\log(\\lambda({\\bf x}))\\)\r\n\\(\\eta({\\bf x}) = \\log \\left(\\frac{p({\\bf x})}{1 - p({\\bf x})} \\right)\\)\r\nMean Function\r\n\\(\\mu({\\bf x}) = \\eta({\\bf x})\\)\r\n\\(\\lambda({\\bf x}) = e^{\\eta({\\bf x})}\\)\r\n\\(p({\\bf x}) = \\frac{e^{\\eta({\\bf x})}}{1 + e^{\\eta({\\bf x})}} = \\frac{1}{1 + e^{-\\eta({\\bf x})}}\\)\r\nLike ordinary linear regression, we will seek to “fit” the model by estimating the \\(\\beta\\) parameters. To do so, we will use the method of maximum likelihood.\r\nNote that a Bernoulli distribution is a specific case of a binomial distribution where the \\(n\\) parameter of a binomial is \\(1\\). Binomial regression is also possible, but we’ll focus on the much more popular Bernoulli case.\r\nSo, in general, GLMs relate the mean of the response to a linear combination of the predictors, \\(\\eta({\\bf x})\\), through the use of a link function, \\(g()\\). That is,\r\n\\[\r\n\\eta({\\bf x}) = g\\left(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\right).\r\n\\]\r\nThe mean is then\r\n\\[\r\n\\text{E}[Y \\mid {\\bf X} = {\\bf x}] = g^{-1}(\\eta({\\bf x})).\r\n\\]\r\nBinary Response\r\nTo illustrate the use of a GLM we’ll focus on the case of binary responses variable coded using \\(0\\) and \\(1\\). In practice, these \\(0\\) and \\(1\\)s will code for two classes such as yes/no, cat/dog, sick/healthy, etc.\r\n\\[\r\nY = \r\n\\begin{cases} \r\n      1 & \\text{yes} \\\\\r\n      0 & \\text{no} \r\n\\end{cases}\r\n\\]\r\nFirst, we define some notation that we will use throughout.\r\n\\[\r\np({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}]\r\n\\]\r\nWith a binary (Bernoulli) response, we’ll mostly focus on the case when \\(Y = 1\\), since with only two possibilities, it is trivial to obtain probabilities when \\(Y = 0\\).\r\n\\[\r\nP[Y = 0 \\mid {\\bf X} = {\\bf x}] + P[Y = 1 \\mid {\\bf X} = {\\bf x}] = 1\r\n\\]\r\n\\[\r\nP[Y = 0 \\mid {\\bf X} = {\\bf x}] = 1 - p({\\bf x})\r\n\\]\r\nWe now define the logistic regression model.\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots  + \\beta_{p - 1} x_{p - 1}\r\n\\]\r\nImmediately we notice some similarities to ordinary linear regression, in particular, the right hand side. This is our usual linear combination of the predictors. We have our usual \\(p - 1\\) predictors for a total of \\(p\\) \\(\\beta\\) parameters. (Note, many more machine learning focused texts will use \\(p\\) as the number of predictors. This is an arbitrary choice, but you should be aware of it.)\r\nThe left hand side is called the log odds, which is the log of the odds. The odds are the probability for a positive event \\((Y = 1)\\) divided by the probability of a negative event \\((Y = 0)\\). So when the odds are \\(1\\), the two events have equal probability. Odds greater than \\(1\\) favor a positive event. The opposite is true when the odds are less than \\(1\\).\r\n\\[\r\n\\frac{p({\\bf x})}{1 - p({\\bf x})} = \\frac{P[Y = 1 \\mid {\\bf X} = {\\bf x}]}{P[Y = 0 \\mid {\\bf X} = {\\bf x}]}\r\n\\]\r\nEssentially, the log odds are the logit transform applied to \\(p({\\bf x})\\).\r\n\\[\r\n\\text{logit}(\\xi) = \\log\\left(\\frac{\\xi}{1 - \\xi}\\right)\r\n\\]\r\nIt will also be useful to define the inverse logit, otherwise known as the “logistic” or sigmoid function.\r\n\\[\r\n\\text{logit}^{-1}(\\xi) = \\frac{e^\\xi}{1 + e^{\\xi}} = \\frac{1}{1 + e^{-\\xi}}\r\n\\]\r\nNote that for \\(x \\in (-\\infty, \\infty))\\), this function outputs values between 0 and 1.\r\nStudents often ask, where is the error term? The answer is that its something that is specific to the normal model. First notice that the model with the error term,\r\n\\[\r\nY = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q + \\epsilon, \\ \\ \\epsilon \\sim N(0, \\sigma^2)\r\n\\] can instead be written as\r\n\\[\r\nY \\mid {\\bf X} = {\\bf x} \\sim N(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q, \\ \\sigma^2).\r\n\\]\r\nWhile our main focus is on estimating the mean, \\(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_qx_q\\), there is also another parameter, \\(\\sigma^2\\) which needs to be estimated. This is the result of the normal distribution having two parameters.\r\nWith logistic regression, which uses the Bernoulli distribution, we only need to estimate the Bernoulli distribution’s single parameter \\(p({\\bf x})\\), which happens to be its mean.\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots  + \\beta_{q} x_{q}\r\n\\]\r\nSo even though we introduced ordinary linear regression first, in some ways, logistic regression is actually simpler.\r\nNote that applying the inverse logit transformation allow us to obtain an expression for \\(p({\\bf x})\\).\r\n\\[\r\np({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}] = \\frac{e^{\\beta_0 + \\beta_1 x_{1} + \\cdots + \\beta_{p-1} x_{(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{1} + \\cdots + \\beta_{p-1} x_{(p-1)}}}\r\n\\]\r\nFitting Logistic Regression\r\nWith \\(n\\) observations, we write the model indexed with \\(i\\) to note that it is being applied to each observation.\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i)})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}\r\n\\]\r\nWe can apply the inverse logit transformation to obtain \\(P[Y_i = 1 \\mid {\\bf X_i} = {\\bf x_i}]\\) for each observation. Since these are probabilities, it’s good that we used a function that returns values between \\(0\\) and \\(1\\).\r\n\\[\r\np({\\bf x_i}) = P[Y_i = 1 \\mid {\\bf X_i} = {\\bf x_i}] = \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}\r\n\\]\r\n\\[\r\n1 - p({\\bf x_i}) = P[Y_i = 0 \\mid {\\bf X} = {\\bf x_i}] = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}\r\n\\]\r\nTo “fit” this model, that is estimate the \\(\\beta\\) parameters, we will use maximum likelihood.\r\n\\[\r\n\\boldsymbol{{\\beta}} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\ldots, \\beta_{p - 1}]\r\n\\]\r\nWe first write the likelihood given the observed data.\r\n\\[\r\nL(\\boldsymbol{{\\beta}}) = \\prod_{i = 1}^{n} P[Y_i = y_i \\mid {\\bf X_i} = {\\bf x_i}]\r\n\\]\r\nThis is already technically a function of the \\(\\beta\\) parameters, but we’ll do some rearrangement to make this more explicit.\r\n\\[\r\nL(\\boldsymbol{{\\beta}}) = \\prod_{i = 1}^{n} p({\\bf x_i})^{y_i} (1 - p({\\bf x_i}))^{(1 - y_i)}\r\n\\]\r\n\\[\r\nL(\\boldsymbol{{\\beta}}) = \\prod_{i : y_i = 1}^{n} p({\\bf x_i}) \\prod_{j : y_j = 0}^{n} (1 - p({\\bf x_j}))\r\n\\]\r\n\\[\r\nL(\\boldsymbol{{\\beta}}) = \\prod_{i : y_i = 1}^{} \\frac{e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}}{1 + e^{\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p-1} x_{i(p-1)}}} \\prod_{j : y_j = 0}^{} \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_{j1} + \\cdots + \\beta_{p-1} x_{j(p-1)}}}\r\n\\]\r\nUnfortunately, unlike ordinary linear regression, there is no analytical solution for this maximization problem. Instead, it will need to be solved numerically. Fortunately, R will take care of this for us using an iteratively reweighted least squares algorithm. (We’ll leave the details for a machine learning or optimization course, which would likely also discuss alternative optimization strategies.)\r\nFitting Issues\r\nWe should note that, if there exists some \\(\\beta^*\\) such that\r\n\\[\r\n{\\bf x_i}^{\\top} \\boldsymbol{{\\beta}^*} > 0 \\implies y_i = 1\r\n\\]\r\nand\r\n\\[\r\n{\\bf x_i}^{\\top} \\boldsymbol{{\\beta}^*} < 0 \\implies y_i = 0\r\n\\]\r\nfor all observations, then the MLE is not unique. Such data is said to be separable.\r\nThis, and similar numeric issues related to estimated probabilities near 0 or 1, will return a warning in R:\r\n\r\n\r\n\r\nWhen this happens, the model is still “fit,” but there are consequences, namely, the estimated coefficients are highly suspect. This is an issue when then trying to interpret the model. When this happens, the model will often still be useful for creating a classifier, which will be discussed later. However, it is still subject to the usual evaluations for classifiers to determine how well it is performing. For details, see Modern Applied Statistics with S-PLUS, Chapter 7.\r\nSimulation Examples\r\n\r\n\r\nsim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3) {\r\n  x = rnorm(n = sample_size)\r\n  eta = beta_0 + beta_1 * x\r\n  p = 1 / (1 + exp(-eta))\r\n  y = rbinom(n = sample_size, size = 1, prob = p)\r\n  data.frame(y, x)\r\n}\r\n\r\n\r\n\r\nYou might think, why not simply use ordinary linear regression? Even with a binary response, our goal is still to model (some function of) \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\). However, with a binary response coded as \\(0\\) and \\(1\\), \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}] = P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) since\r\n\\[\r\n\\begin{aligned}\r\n\\text{E}[Y \\mid {\\bf X} = {\\bf x}] &=  1 \\cdot P[Y = 1 \\mid {\\bf X} = {\\bf x}] + 0 \\cdot P[Y = 0 \\mid {\\bf X} = {\\bf x}] \\\\\r\n                                  &= P[Y = 1 \\mid {\\bf X} = {\\bf x}]\r\n\\end{aligned}\r\n\\]\r\nThen why can’t we just use ordinary linear regression to estimate \\(\\text{E}[Y \\mid {\\bf X} = {\\bf x}]\\), and thus \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\)?\r\nTo investigate, let’s simulate data from the following model:\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -2 + 3 x\r\n\\]\r\nAnother way to write this, which better matches the function we’re using to simulate the data:\r\n\\[\r\n\\begin{aligned}\r\nY_i \\mid {\\bf X_i} = {\\bf x_i} &\\sim \\text{Bern}(p_i) \\\\\r\np_i &= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\\r\n\\eta({\\bf x_i}) &= -2 + 3 x_i\r\n\\end{aligned}\r\n\\]\r\n\r\n\r\nset.seed(1)\r\nexample_data = sim_logistic_data()\r\nhead(example_data)\r\n\r\n\r\n  y          x\r\n1 0 -0.6264538\r\n2 1  0.1836433\r\n3 0 -0.8356286\r\n4 1  1.5952808\r\n5 0  0.3295078\r\n6 0 -0.8204684\r\n\r\nAfter simulating a dataset, we’ll then fit both ordinary linear regression and logistic regression. Notice that currently the responses variable y is a numeric variable that only takes values 0 and 1. Later we’ll see that we can also fit logistic regression when the response is a factor variable with only two levels. (Generally, having a factor response is preferred, but having a dummy response allows use to make the comparison to using ordinary linear regression.)\r\n\r\n\r\n# ordinary linear regression\r\nfit_lm  = lm(y ~ x, data = example_data)\r\n# logistic regression\r\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\r\n\r\n\r\n\r\nNotice that the syntax is extremely similar. What’s changed?\r\nlm() has become glm()\r\nWe’ve added family = binomial argument\r\nIn a lot of ways, lm() is just a more specific version of glm(). For example\r\n\r\n\r\nglm(y ~ x, data = example_data)\r\n\r\n\r\n\r\nwould actually fit the ordinary linear regression that we have seen in the past. By default, glm() uses family = gaussian argument. That is, we’re fitting a GLM with a normally distributed response and the identity function as the link.\r\nThe family argument to glm() actually specifies both the distribution and the link function. If not made explicit, the link function is chosen to be the canonical link function, which is essentially the most mathematical convenient link function. See ?glm and ?family for details. For example, the following code explicitly specifies the link function which was previously used by default.\r\n\r\n\r\n# more detailed call to glm for logistic regression\r\nfit_glm = glm(y ~ x, data = example_data, family = binomial(link = \"logit\"))\r\n\r\n\r\n\r\nMaking predictions with an object of type glm is slightly different than making predictions after fitting with lm(). In the case of logistic regression, with family = binomial, we have:\r\ntype\r\nReturned\r\n\"link\" [default]\r\n\\(\\hat{\\eta}({\\bf x}) = \\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right)\\)\r\n\"response\"\r\n\\(\\hat{p}({\\bf x}) = \\frac{e^{\\hat{\\eta}({\\bf x})}}{1 + e^{\\hat{\\eta}({\\bf x})}} = \\frac{1}{1 + e^{-\\hat{\\eta}({\\bf x})}}\\)\r\nThat is, type = \"link\" will get you the log odds, while type = \"response\" will return the estimated mean, in this case, \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\) for each observation.\r\n\r\n\r\nplot(y ~ x, data = example_data, \r\n     pch = 20, ylab = \"Estimated Probability\", \r\n     main = \"Ordinary vs Logistic Regression\")\r\ngrid()\r\nabline(fit_lm, col = \"darkorange\")\r\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \r\n      add = TRUE, col = \"dodgerblue\", lty = 2)\r\nlegend(\"topleft\", c(\"Ordinary\", \"Logistic\", \"Data\"), lty = c(1, 2, 0), \r\n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))\r\n\r\n\r\n\r\n\r\nSince we only have a single predictor variable, we are able to graphically show this situation. First, note that the data, is plotted using black dots. The response y only takes values 0 and 1.\r\nNext, we need to discuss the two added lines to the plot. The first, the solid orange line, is the fitted ordinary linear regression.\r\nThe dashed blue curve is the estimated logistic regression. It is helpful to realize that we are not plotting an estimate of \\(Y\\) for either. (Sometimes it might seem that way with ordinary linear regression, but that isn’t what is happening.) For both, we are plotting \\(\\hat{\\text{E}}[Y \\mid {\\bf X} = {\\bf x}]\\), the estimated mean, which for a binary response happens to be an estimate of \\(P[Y = 1 \\mid {\\bf X} = {\\bf x}]\\).\r\nWe immediately see why ordinary linear regression is not a good idea. While it is estimating the mean, we see that it produces estimates that are less than 0! (And in other situations could produce estimates greater than 1!) If the mean is a probability, we don’t want probabilities less than 0 or greater than 1.\r\nEnter logistic regression. Since the output of the inverse logit function is restricted to be between 0 and 1, our estimates make much more sense as probabilities. Let’s look at our estimated coefficients. (With a lot of rounding, for simplicity.)\r\n\r\n\r\nround(coef(fit_glm), 1)\r\n\r\n\r\n(Intercept)           x \r\n       -2.3         3.7 \r\n\r\nOur estimated model is then:\r\n\\[\r\n\\log\\left(\\frac{\\hat{p}({\\bf x})}{1 - \\hat{p}({\\bf x})}\\right) = -2.3 + 3.7 x\r\n\\]\r\nBecause we’re not directly estimating the mean, but instead a function of the mean, we need to be careful with our interpretation of \\(\\hat{\\beta}_1 = 3.7\\). This means that, for a one unit increase in \\(x\\), the log odds change (in this case increase) by \\(3.7\\). Also, since \\(\\hat{\\beta}_1\\) is positive, as we increase \\(x\\) we also increase \\(\\hat{p}({\\bf x})\\). To see how much, we have to consider the inverse logistic function.\r\nFor example, we have:\r\n\\[\r\n\\hat{P}[Y = 1 \\mid X = -0.5] = \\frac{e^{-2.3 + 3.7 \\cdot (-0.5)}}{1 + e^{-2.3 + 3.7 \\cdot (-0.5)}} \\approx 0.016\r\n\\]\r\n\\[\r\n\\hat{P}[Y = 1 \\mid X = 0] = \\frac{e^{-2.3 + 3.7 \\cdot (0)}}{1 + e^{-2.3 + 3.7 \\cdot (0)}} \\approx 0.09112296\r\n\\]\r\n\\[\r\n\\hat{P}[Y = 1 \\mid X = 1] = \\frac{e^{-2.3 + 3.7 \\cdot (1)}}{1 + e^{-2.3 + 3.7 \\cdot (1)}} \\approx 0.8021839\r\n\\]\r\nNow that we know we should use logistic regression, and not ordinary linear regression, let’s consider another example. This time, let’s consider the model\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = 1 + -4 x.\r\n\\]\r\nAgain, we could re-write this to better match the function we’re using to simulate the data:\r\n\\[\r\n\\begin{aligned}\r\nY_i \\mid {\\bf X_i} = {\\bf x_i} &\\sim \\text{Bern}(p_i) \\\\\r\np_i &= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\\r\n\\eta({\\bf x_i}) &= 1 + -4 x_i\r\n\\end{aligned}\r\n\\]\r\nIn this model, as \\(x\\) increases, the log odds decrease.\r\n\r\n\r\nset.seed(1)\r\nexample_data = sim_logistic_data(sample_size = 50, beta_0 = 1, beta_1 = -4)\r\n\r\n\r\n\r\nWe again simulate some observations form this model, then fit logistic regression.\r\n\r\n\r\nfit_glm = glm(y ~ x, data = example_data, family = binomial)\r\n\r\n\r\n\r\n\r\n\r\nplot(y ~ x, data = example_data, \r\n     pch = 20, ylab = \"Estimated Probability\", \r\n     main = \"Logistic Regression, Decreasing Probability\")\r\ngrid()\r\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \r\n      add = TRUE, col = \"dodgerblue\", lty = 2)\r\ncurve(boot::inv.logit(1 - 4 * x), add = TRUE, col = \"darkorange\", lty = 1)\r\nlegend(\"bottomleft\", c(\"True Probability\", \"Estimated Probability\", \"Data\"), lty = c(1, 2, 0), \r\n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))\r\n\r\n\r\n\r\n\r\nWe see that this time, as \\(x\\) increases, \\(\\hat{p}({\\bf x})\\) decreases.\r\nNow let’s look at an example where the estimated probability doesn’t always simply increase or decrease. Much like ordinary linear regression, the linear combination of predictors can contain transformations of predictors (in this case a quadratic term) and interactions.\r\n\r\n\r\nsim_quadratic_logistic_data = function(sample_size = 25) {\r\n  x = rnorm(n = sample_size)\r\n  eta = -1.5 + 0.5 * x + x ^ 2\r\n  p = 1 / (1 + exp(-eta))\r\n  y = rbinom(n = sample_size, size = 1, prob = p)\r\n  data.frame(y, x)\r\n}\r\n\r\n\r\n\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = -1.5 + 0.5x + x^2.\r\n\\]\r\nAgain, we could re-write this to better match the function we’re using to simulate the data:\r\n\\[\r\n\\begin{aligned}\r\nY_i \\mid {\\bf X_i} = {\\bf x_i} &\\sim \\text{Bern}(p_i) \\\\\r\np_i &= p({\\bf x_i}) = \\frac{1}{1 + e^{-\\eta({\\bf x_i})}} \\\\\r\n\\eta({\\bf x_i}) &= -1.5 + 0.5x_i + x_i^2\r\n\\end{aligned}\r\n\\]\r\n\r\n\r\nset.seed(42)\r\nexample_data = sim_quadratic_logistic_data(sample_size = 50)\r\n\r\n\r\n\r\n\r\n\r\nfit_glm = glm(y ~ x + I(x^2), data = example_data, family = binomial)\r\n\r\n\r\n\r\n\r\n\r\nplot(y ~ x, data = example_data, \r\n     pch = 20, ylab = \"Estimated Probability\", \r\n     main = \"Logistic Regression, Quadratic Relationship\")\r\ngrid()\r\ncurve(predict(fit_glm, data.frame(x), type = \"response\"), \r\n      add = TRUE, col = \"dodgerblue\", lty = 2)\r\ncurve(boot::inv.logit(-1.5 + 0.5 * x + x ^ 2), \r\n      add = TRUE, col = \"darkorange\", lty = 1)\r\nlegend(\"bottomleft\", c(\"True Probability\", \"Estimated Probability\", \"Data\"), lty = c(1, 2, 0), \r\n       pch = c(NA, NA, 20), lwd = 2, col = c(\"darkorange\", \"dodgerblue\", \"black\"))\r\n\r\n\r\n\r\n\r\nWorking with Logistic Regression\r\nWhile the logistic regression model isn’t exactly the same as the ordinary linear regression model, because they both use a linear combination of the predictors\r\n\\[\r\n\\eta({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots  + \\beta_{p - 1} x_{p - 1}\r\n\\]\r\nworking with logistic regression is very similar. Many of the things we did with ordinary linear regression can be done with logistic regression in a very similar fashion. For example,\r\nTesting for a single \\(\\beta\\) parameter\r\nTesting for a set of \\(\\beta\\) parameters\r\nFormula specification in R\r\nInterpreting parameters and estimates\r\nConfidence intervals for parameters\r\nConfidence intervals for mean response\r\nVariable selection\r\nAfter some introduction to the new tests, we’ll demonstrate each of these using an example.\r\nTesting with GLMs\r\nLike ordinary linear regression, we’ll want to be able to perform hypothesis testing. We’ll again want both single parameter, and multiple parameter tests.\r\nWald Test\r\nIn ordinary linear regression, we performed the test of\r\n\\[\r\nH_0: \\beta_j = 0 \\quad \\text{vs} \\quad H_1: \\beta_j \\neq 0\r\n\\]\r\nusing a \\(t\\)-test.\r\nFor the logistic regression model,\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\ldots  + \\beta_{p - 1} x_{p - 1}\r\n\\]\r\nwe can again perform a test of\r\n\\[\r\nH_0: \\beta_j = 0 \\quad \\text{vs} \\quad H_1: \\beta_j \\neq 0\r\n\\]\r\nhowever, the test statistic and its distribution are no longer \\(t\\). We see that the test statistic takes the same form\r\n\\[\r\nz = \\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}[\\hat{\\beta}_j]} \\overset{\\text{approx}}{\\sim} N(0, 1)\r\n\\]\r\nbut now we are performing a \\(z\\)-test, as the test statistic is approximated by a standard normal distribution, provided we have a large enough sample. (The \\(t\\)-test for ordinary linear regression, assuming the assumptions were correct, had an exact distribution for any sample size.)\r\nWe’ll skip some of the exact details of the calculations, as R will obtain the standard error for us. The use of this test will be extremely similar to the \\(t\\)-test for ordinary linear regression. Essentially the only thing that changes is the distribution of the test statistic.\r\nLikelihood-Ratio Test\r\nConsider the following full model,\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(p-1)} x_{i(p-1)} + \\epsilon_i\r\n\\]\r\nThis model has \\(p - 1\\) predictors, for a total of \\(p\\) \\(\\beta\\)-parameters. We will denote the MLE of these \\(\\beta\\)-parameters as \\(\\hat{\\beta}_{\\text{Full}}\\)\r\nNow consider a null (or reduced) model,\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x_i})}{1 - p({\\bf x_i})}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_{(q-1)} x_{i(q-1)} + \\epsilon_i\r\n\\]\r\nwhere \\(q < p\\). This model has \\(q - 1\\) predictors, for a total of \\(q\\) \\(\\beta\\)-parameters. We will denote the MLE of these \\(\\beta\\)-parameters as \\(\\hat{\\beta}_{\\text{Null}}\\)\r\nThe difference between these two models can be codified by the null hypothesis of a test.\r\n\\[\r\nH_0: \\beta_q = \\beta_{q+1} = \\cdots = \\beta_{p - 1} = 0.\r\n\\]\r\nThis implies that the reduced model is nested inside the full model.\r\nWe then define a test statistic, \\(D\\),\r\n\\[\r\nD = -2 \\log \\left( \\frac{L(\\boldsymbol{\\hat{\\beta}_{\\text{Null}}})} {L(\\boldsymbol{\\hat{\\beta}_{\\text{Full}}})} \\right) = 2 \\log \\left( \\frac{L(\\boldsymbol{\\hat{\\beta}_{\\text{Full}}})} {L(\\boldsymbol{\\hat{\\beta}_{\\text{Null}}})} \\right) = 2 \\left( \\ell(\\hat{\\beta}_{\\text{Full}}) - \\ell(\\hat{\\beta}_{\\text{Null}})\\right)\r\n\\]\r\nwhere \\(L\\) denotes a likelihood and \\(\\ell\\) denotes a log-likelihood. For a large enough sample, this test statistic has an approximate Chi-square distribution\r\n\\[\r\nD \\overset{\\text{approx}}{\\sim} \\chi^2_{k}\r\n\\]\r\nwhere \\(k = p - q\\), the difference in number of parameters of the two models.\r\nThis test, which we will call the Likelihood-Ratio Test, will be the analogue to the ANOVA \\(F\\)-test for logistic regression. Interestingly, to perform the Likelihood-Ratio Test, we’ll actually again use the anova() function in R!.\r\nThe Likelihood-Ratio Test is actually a rather general test, however, here we have presented a specific application to nested logistic regression models.\r\nSAheart Example\r\nTo illustrate the use of logistic regression, we will use the SAheart dataset from the ElemStatLearn package.\r\n\r\n\r\n# install.packages(\"bestglm\")\r\nlibrary(bestglm)\r\ndata(\"SAheart\")\r\n\r\n\r\n\r\n\r\nsbp\r\ntobacco\r\nldl\r\nadiposity\r\nfamhist\r\ntypea\r\nobesity\r\nalcohol\r\nage\r\nchd\r\n160\r\n12.00\r\n5.73\r\n23.11\r\nPresent\r\n49\r\n25.30\r\n97.20\r\n52\r\n1\r\n144\r\n0.01\r\n4.41\r\n28.61\r\nAbsent\r\n55\r\n28.87\r\n2.06\r\n63\r\n1\r\n118\r\n0.08\r\n3.48\r\n32.28\r\nPresent\r\n52\r\n29.14\r\n3.81\r\n46\r\n0\r\n170\r\n7.50\r\n6.41\r\n38.03\r\nPresent\r\n51\r\n31.99\r\n24.26\r\n58\r\n1\r\n134\r\n13.60\r\n3.50\r\n27.78\r\nPresent\r\n60\r\n25.99\r\n57.34\r\n49\r\n1\r\n132\r\n6.20\r\n6.47\r\n36.21\r\nPresent\r\n62\r\n30.77\r\n14.14\r\n45\r\n0\r\n\r\nThis data comes from a retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa. The chd variable, which we will use as a response, indicates whether or not coronary heart disease is present in an individual. Note that this is coded as a numeric 0 / 1 variable. Using this as a response with glm() it is important to indicate family = binomial, otherwise ordinary linear regression will be fit. Later, we will see the use of a factor variable response, which is actually preferred, as you cannot accidentally fit ordinary linear regression.\r\nThe predictors are various measurements for each individual, many related to heart health. For example sbp, systolic blood pressure, and ldl, low density lipoprotein cholesterol. For full details, use ?SAheart.\r\nWe’ll begin by attempting to model the probability of coronary heart disease based on low density lipoprotein cholesterol. That is, we will fit the model\r\n\\[\r\n\\log\\left(\\frac{P[\\texttt{chd} = 1]}{1 - P[\\texttt{chd} = 1]}\\right) = \\beta_0 + \\beta_{\\texttt{ldl}} x_{\\texttt{ldl}}\r\n\\]\r\n\r\n\r\nchd_mod_ldl = glm(chd ~ ldl, data = SAheart, family = binomial)\r\nplot(jitter(chd, factor = 0.1) ~ ldl, data = SAheart, pch = 20, \r\n     ylab = \"Probability of CHD\", xlab = \"Low Density Lipoprotein Cholesterol\")\r\ngrid()\r\ncurve(predict(chd_mod_ldl, data.frame(ldl = x), type = \"response\"), \r\n      add = TRUE, col = \"dodgerblue\", lty = 2)\r\n\r\n\r\n\r\n\r\nAs before, we plot the data in addition to the estimated probabilities. Note that we have “jittered” the data to make it easier to visualize, but the data do only take values 0 and 1.\r\nAs we would expect, this plot indicates that as ldl increases, so does the probability of chd.\r\n\r\n\r\ncoef(summary(chd_mod_ldl))\r\n\r\n\r\n              Estimate Std. Error   z value     Pr(>|z|)\r\n(Intercept) -1.9686681 0.27307908 -7.209150 5.630207e-13\r\nldl          0.2746613 0.05163983  5.318787 1.044615e-07\r\n\r\nTo perform the test\r\n\\[\r\nH_0: \\beta_{\\texttt{ldl}} = 0\r\n\\]\r\nwe use the summary() function as we have done so many times before. Like the \\(t\\)-test for ordinary linear regression, this returns the estimate of the parameter, its standard error, the relevant test statistic (\\(z\\)), and its p-value. Here we have an incredibly low p-value, so we reject the null hypothesis. The ldl variable appears to be a significant predictor.\r\nWhen fitting logistic regression, we can use the same formula syntax as ordinary linear regression. So, to fit an additive model using all available predictors, we use:\r\n\r\n\r\nchd_mod_additive = glm(chd ~ ., data = SAheart, family = binomial)\r\n\r\n\r\n\r\nWe can then use the likelihood-ratio test to compare the two models. Specifically, we are testing\r\n\\[\r\nH_0: \\beta_{\\texttt{sbp}} = \\beta_{\\texttt{tobacco}} = \\beta_{\\texttt{adiposity}} = \\beta_{\\texttt{famhist}} = \\beta_{\\texttt{typea}} = \\beta_{\\texttt{obesity}} = \\beta_{\\texttt{alcohol}} = \\beta_{\\texttt{age}} = 0\r\n\\]\r\nWe could manually calculate the test statistic,\r\n\r\n\r\n-2 * as.numeric(logLik(chd_mod_ldl) - logLik(chd_mod_additive))\r\n\r\n\r\n[1] 92.13879\r\n\r\nOr we could utilize the anova() function. By specifying test = \"LRT\", R will use the likelihood-ratio test to compare the two models.\r\n\r\n\r\nanova(chd_mod_ldl, chd_mod_additive, test = \"LRT\")\r\n\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel 1: chd ~ ldl\r\nModel 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + \r\n    alcohol + age\r\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \r\n1       460     564.28                          \r\n2       452     472.14  8   92.139 < 2.2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nWe see that the test statistic that we had just calculated appears in the output. The very small p-value suggests that we prefer the larger model.\r\nWhile we prefer the additive model compared to the model with only a single predictor, do we actually need all of the predictors in the additive model? To select a subset of predictors, we can use a stepwise procedure as we did with ordinary linear regression. Recall that AIC and BIC were defined in terms of likelihoods. Here we demonstrate using AIC with a backwards selection procedure.\r\n\r\n\r\nchd_mod_selected = step(chd_mod_additive, trace = 0)\r\ncoef(chd_mod_selected)\r\n\r\n\r\n   (Intercept)        tobacco            ldl famhistPresent \r\n   -6.44644451     0.08037533     0.16199164     0.90817526 \r\n         typea            age \r\n    0.03711521     0.05046038 \r\n\r\nWe could again compare this model to the additive models.\r\n\\[\r\nH_0: \\beta_{\\texttt{sbp}} = \\beta_{\\texttt{adiposity}} = \\beta_{\\texttt{obesity}} = \\beta_{\\texttt{alcohol}} = 0\r\n\\]\r\n\r\n\r\nanova(chd_mod_selected, chd_mod_additive, test = \"LRT\")\r\n\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel 1: chd ~ tobacco + ldl + famhist + typea + age\r\nModel 2: chd ~ sbp + tobacco + ldl + adiposity + famhist + typea + obesity + \r\n    alcohol + age\r\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\r\n1       456     475.69                     \r\n2       452     472.14  4   3.5455    0.471\r\n\r\nHere it seems that we would prefer the selected model.\r\nConfidence Intervals\r\nWe can create confidence intervals for the \\(\\beta\\) parameters using the confint() function as we did with ordinary linear regression.\r\n\r\n\r\nconfint(chd_mod_selected, level = 0.99)\r\n\r\n\r\n                      0.5 %      99.5 %\r\n(Intercept)    -8.941825274 -4.18278990\r\ntobacco         0.015704975  0.14986616\r\nldl             0.022923610  0.30784590\r\nfamhistPresent  0.330033483  1.49603366\r\ntypea           0.006408724  0.06932612\r\nage             0.024847330  0.07764277\r\n\r\nNote that we could create intervals by rearranging the results of the Wald test to obtain the Wald confidence interval. This would be given by\r\n\\[\r\n\\hat{\\beta}_j \\pm z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\beta}_j].\r\n\\]\r\nHowever, R is using a slightly different approach based on a concept called the profile likelihood. (The details of which we will omit.) Ultimately the intervals reported will be similar, but the method used by R is more common in practice, probably at least partially because it is the default approach in R. Check to see how intervals using the formula above compare to those from the output of confint(). (Or, note that using confint.default() will return the results of calculating the Wald confidence interval.)\r\nConfidence Intervals for Mean Response\r\nConfidence intervals for the mean response require some additional thought. With a “large enough” sample, we have\r\n\\[\r\n\\frac{\\hat{\\eta}({\\bf x}) - \\eta({\\bf x})}{\\text{SE}[\\hat{\\eta}({\\bf x})]} \\overset{\\text{approx}}{\\sim} N(0, 1)\r\n\\]\r\nThen we can create an approximate \\((1 - \\alpha)\\%\\) confidence intervals for \\(\\eta({\\bf x})\\) using\r\n\\[\r\n\\hat{\\eta}({\\bf x}) \\pm z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})]\r\n\\]\r\nwhere \\(z_{\\alpha/2}\\) is the critical value such that \\(P(Z > z_{\\alpha/2}) = \\alpha/2\\).\r\nThis isn’t a particularly interesting interval. Instead, what we really want is an interval for the mean response, \\(p({\\bf x})\\). To obtain an interval for \\(p({\\bf x})\\), we simply apply the inverse logit transform to the endpoints of the interval for \\(\\eta.\\)\r\n\\[\r\n\\left(\\text{logit}^{-1}(\\hat{\\eta}({\\bf x}) - z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})] ), \\ \\text{logit}^{-1}(\\hat{\\eta}({\\bf x}) + z_{\\alpha/2} \\cdot \\text{SE}[\\hat{\\eta}({\\bf x})])\\right)\r\n\\]\r\nTo demonstrate creating these intervals, we’ll consider a new observation.\r\n\r\n\r\nnew_obs = data.frame(\r\n  sbp = 148.0,\r\n  tobacco = 5,\r\n  ldl = 12,\r\n  adiposity = 31.23,\r\n  famhist = \"Present\",\r\n  typea = 47,\r\n  obesity = 28.50,\r\n  alcohol = 23.89,\r\n  age = 60\r\n)\r\n\r\n\r\n\r\nFist, we’ll use the predict() function to obtain \\(\\hat{\\eta}({\\bf x})\\) for this observation.\r\n\r\n\r\neta_hat = predict(chd_mod_selected, new_obs, se.fit = TRUE, type = \"link\")\r\neta_hat\r\n\r\n\r\n$fit\r\n       1 \r\n1.579545 \r\n\r\n$se.fit\r\n[1] 0.4114796\r\n\r\n$residual.scale\r\n[1] 1\r\n\r\nBy setting se.fit = TRUE, R also computes \\(\\text{SE}[\\hat{\\eta}({\\bf x})]\\). Note that we used type = \"link\", but this is actually a default value. We added it here to stress that the output from predict() will be the value of the link function.\r\n\r\n\r\nz_crit = round(qnorm(0.975), 2)\r\nround(z_crit, 2)\r\n\r\n\r\n[1] 1.96\r\n\r\nAfter obtaining the correct critical value, we can easily create a \\(95\\%\\) confidence interval for \\(\\eta({\\bf x})\\).\r\n\r\n\r\neta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit\r\n\r\n\r\n[1] 0.773045 2.386045\r\n\r\nNow we simply need to apply the correct transformation to make this a confidence interval for \\(p({\\bf x})\\), the probability of coronary heart disease for this observation. Note that the boot package contains functions logit() and inv.logit() which are the logit and inverse logit transformations, respectively.\r\n\r\n\r\nboot::inv.logit(eta_hat$fit + c(-1, 1) * z_crit * eta_hat$se.fit)\r\n\r\n\r\n[1] 0.6841792 0.9157570\r\n\r\nNotice, as we would expect, the bounds of this interval are both between 0 and 1. Also, since both bounds of the interval for \\(\\eta({\\bf x})\\) are positive, both bounds of the interval for \\(p({\\bf x})\\) are greater than 0.5.\r\nFormula Syntax\r\nWithout really thinking about it, we’ve been using our previous knowledge of R’s model formula syntax to fit logistic regression.\r\nInteractions\r\nLet’s add an interaction between LDL and family history for the model we selected.\r\n\r\n\r\nchd_mod_interaction = glm(chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist, \r\n                          data = SAheart, family = binomial)\r\nsummary(chd_mod_interaction)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist, \r\n    family = binomial, data = SAheart)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-1.9082  -0.8308  -0.4550   0.9286   2.5152  \r\n\r\nCoefficients:\r\n                    Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)        -6.043472   0.937186  -6.449 1.13e-10 ***\r\nalcohol             0.003800   0.004332   0.877  0.38033    \r\nldl                 0.035593   0.071448   0.498  0.61837    \r\nfamhistPresent     -0.733836   0.618131  -1.187  0.23515    \r\ntypea               0.036253   0.012172   2.978  0.00290 ** \r\nage                 0.062416   0.009723   6.419 1.37e-10 ***\r\nldl:famhistPresent  0.314311   0.114922   2.735  0.00624 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 596.11  on 461  degrees of freedom\r\nResidual deviance: 477.46  on 455  degrees of freedom\r\nAIC: 491.46\r\n\r\nNumber of Fisher Scoring iterations: 5\r\n\r\nBased on the \\(z\\)-test seen in the above summary, this interaction is significant. The effect of LDL on the probability of CHD is different depending on family history.\r\nPolynomial Terms\r\nLet’s take the previous model, and now add a polynomial term.\r\n\r\n\r\nchd_mod_int_quad = glm(chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist + I(ldl^2),\r\n                       data = SAheart, family = binomial)\r\nsummary(chd_mod_int_quad)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = chd ~ alcohol + ldl + famhist + typea + age + ldl:famhist + \r\n    I(ldl^2), family = binomial, data = SAheart)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-1.8953  -0.8311  -0.4556   0.9276   2.5204  \r\n\r\nCoefficients:\r\n                    Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)        -6.096747   1.065952  -5.720 1.07e-08 ***\r\nalcohol             0.003842   0.004350   0.883  0.37716    \r\nldl                 0.056876   0.214420   0.265  0.79081    \r\nfamhistPresent     -0.723769   0.625167  -1.158  0.24698    \r\ntypea               0.036248   0.012171   2.978  0.00290 ** \r\nage                 0.062299   0.009788   6.365 1.95e-10 ***\r\nI(ldl^2)           -0.001587   0.015076  -0.105  0.91617    \r\nldl:famhistPresent  0.311615   0.117559   2.651  0.00803 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 596.11  on 461  degrees of freedom\r\nResidual deviance: 477.45  on 454  degrees of freedom\r\nAIC: 493.45\r\n\r\nNumber of Fisher Scoring iterations: 5\r\n\r\nUnsurprisingly, since this additional transformed variable wasn’t intelligently chosen, it is not significant. However, this does allow us to stress the fact that the syntax notation that we had been using with lm() works basically exactly the same for glm(), however now we understand that this is specifying the linear combination of predictions, \\(\\eta({\\bf x})\\).\r\nThat is, the above fits the model\r\n\\[\r\n\\log\\left(\\frac{p({\\bf x})}{1 - p({\\bf x})}\\right) = \r\n\\beta_0 +\r\n\\beta_{1}x_{\\texttt{alcohol}} +\r\n\\beta_{2}x_{\\texttt{ldl}} +\r\n\\beta_{3}x_{\\texttt{famhist}} +\r\n\\beta_{4}x_{\\texttt{typea}} +\r\n\\beta_{5}x_{\\texttt{age}} +\r\n\\beta_{6}x_{\\texttt{ldl}}x_{\\texttt{famhist}} +\r\n\\beta_{7}x_{\\texttt{ldl}}^2\r\n\\]\r\nYou may have realized this before we actually explicitly wrote it down!\r\nDeviance\r\nYou have probably noticed that the output from summary() is also very similar to that of ordinary linear regression. One difference, is the “deviance” being reported. The Null deviance is the deviance for the null model, that is, a model with no predictors. The Residual deviance is the deviance for the model that was fit.\r\nDeviance compares the model to a saturated model. (Without repeated observations, a saturated model is a model that fits perfectly, using a parameter for each observation.) Essentially, deviance is a generalized residual sum of squares for GLMs. Like RSS, deviance decreased as the model complexity increases.\r\n\r\n\r\ndeviance(chd_mod_ldl)\r\n\r\n\r\n[1] 564.2788\r\n\r\ndeviance(chd_mod_selected)\r\n\r\n\r\n[1] 475.6856\r\n\r\ndeviance(chd_mod_additive)\r\n\r\n\r\n[1] 472.14\r\n\r\nNote that these are nested, and we see that deviance does decrease as the model size becomes larger. So while a lower deviance is better, if the model becomes too big, it may be overfitting. Note that R also outputs AIC in the summary, which will penalize according to model size, to prevent overfitting.\r\nClassification\r\nSo far we’ve mostly used logistic regression to estimate class probabilities. The somewhat obvious next step is to use these probabilities to make “predictions,” which in this context, we would call classifications. Based on the values of the predictors, should an observation be classified as \\(Y = 1\\) or as \\(Y = 0\\)?\r\nSuppose we didn’t need to estimate probabilities from data, and instead, we actually knew both\r\n\\[\r\np({\\bf x}) = P[Y = 1 \\mid {\\bf X} = {\\bf x}]\r\n\\]\r\nand\r\n\\[\r\n1 - p({\\bf x}) = P[Y = 0 \\mid {\\bf X} = {\\bf x}].\r\n\\]\r\nWith this information, classifying observations based on the values of the predictors is actually extremely easy. Simply classify an observation to the class (\\(0\\) or \\(1\\)) with the larger probability. In general, this result is called the Bayes Classifier,\r\n\\[\r\nC^B({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ P[Y = k \\mid {\\bf X = x}].\r\n\\]\r\nFor a binary response, that is,\r\n\\[\r\n\\hat{C}(\\bf x) = \r\n\\begin{cases} \r\n      1 & p({\\bf x}) > 0.5 \\\\\r\n      0 & p({\\bf x}) \\leq 0.5 \r\n\\end{cases}\r\n\\]\r\nSimply put, the Bayes classifier (not to be confused with the Naive Bayes Classifier) minimizes the probability of misclassification by classifying each observation to the class with the highest probability. Unfortunately, in practice, we won’t know the necessary probabilities to directly use the Bayes classifier. Instead we’ll have to use estimated probabilities. So to create a classifier that seeks to minimize misclassifications, we would use,\r\n\\[\r\n\\hat{C}({\\bf x}) = \\underset{k}{\\mathrm{argmax}} \\ \\hat{P}[Y = k \\mid {\\bf X = x}].\r\n\\]\r\nIn the case of a binary response since \\(\\hat{p}({\\bf x}) = 1 - \\hat{p}({\\bf x})\\), this becomes\r\n\\[\r\n\\hat{C}(\\bf x) = \r\n\\begin{cases} \r\n      1 & \\hat{p}({\\bf x}) > 0.5 \\\\\r\n      0 & \\hat{p}({\\bf x}) \\leq 0.5 \r\n\\end{cases}\r\n\\]\r\nUsing this simple classification rule, we can turn logistic regression into a classifier. To use logistic regression for classification, we first use logistic regression to obtain estimated probabilities, \\(\\hat{p}({\\bf x})\\), then use these in conjunction with the above classification rule.\r\nLogistic regression is just one of many ways that these probabilities could be estimated. In a course completely focused on machine learning, you’ll learn many additional ways to do this, as well as methods to directly make classifications without needing to first estimate probabilities. But since we had already introduced logistic regression, it makes sense to discuss it in the context of classification.\r\nspam Example\r\nTo illustrate the use of logistic regression as a classifier, we will use the spam dataset from the kernlab package.\r\n\r\n\r\n# install.packages(\"kernlab\")\r\nlibrary(kernlab)\r\ndata(\"spam\")\r\ntibble::as.tibble(spam)\r\n\r\n\r\n# A tibble: 4,601 x 58\r\n    make address   all num3d   our  over remove internet order  mail\r\n   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl> <dbl> <dbl>\r\n 1  0       0.64  0.64     0  0.32  0      0        0     0     0   \r\n 2  0.21    0.28  0.5      0  0.14  0.28   0.21     0.07  0     0.94\r\n 3  0.06    0     0.71     0  1.23  0.19   0.19     0.12  0.64  0.25\r\n 4  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63\r\n 5  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63\r\n 6  0       0     0        0  1.85  0      0        1.85  0     0   \r\n 7  0       0     0        0  1.92  0      0        0     0     0.64\r\n 8  0       0     0        0  1.88  0      0        1.88  0     0   \r\n 9  0.15    0     0.46     0  0.61  0      0.3      0     0.92  0.76\r\n10  0.06    0.12  0.77     0  0.19  0.32   0.38     0     0.06  0   \r\n# ... with 4,591 more rows, and 48 more variables: receive <dbl>,\r\n#   will <dbl>, people <dbl>, report <dbl>, addresses <dbl>,\r\n#   free <dbl>, business <dbl>, email <dbl>, you <dbl>, credit <dbl>,\r\n#   your <dbl>, font <dbl>, num000 <dbl>, money <dbl>, hp <dbl>,\r\n#   hpl <dbl>, george <dbl>, num650 <dbl>, lab <dbl>, labs <dbl>,\r\n#   telnet <dbl>, num857 <dbl>, data <dbl>, num415 <dbl>,\r\n#   num85 <dbl>, technology <dbl>, num1999 <dbl>, parts <dbl>, ...\r\n\r\nThis dataset, created in the late 1990s at Hewlett-Packard Labs, contains 4601 emails, of which 1813 are considered spam. The remaining are not spam. (Which for simplicity, we might call, ham.) Additional details can be obtained by using ?spam of by visiting the UCI Machine Learning Repository.\r\nThe response variable, type, is a factor with levels that label each email as spam or nonspam. When fitting models, nonspam will be the reference level, \\(Y = 0\\), as it comes first alphabetically.\r\n\r\n\r\nis.factor(spam$type)\r\n\r\n\r\n[1] TRUE\r\n\r\nlevels(spam$type)\r\n\r\n\r\n[1] \"nonspam\" \"spam\"   \r\n\r\nMany of the predictors (often called features in machine learning) are engineered based on the emails. For example, charDollar is the number of times an email contains the $ character. Some variables are highly specific to this dataset, for example george and num650. (The name and area code for one of the researchers whose emails were used.) We should keep in mind that this dataset was created based on emails send to academic type researcher in the 1990s. Any results we derive probably won’t generalize to modern emails for the general public.\r\nTo get started, we’ll first test-train split the data.\r\n\r\n\r\nset.seed(42)\r\n# spam_idx = sample(nrow(spam), round(nrow(spam) / 2))\r\nspam_idx = sample(nrow(spam), 1000)\r\nspam_trn = spam[spam_idx, ]\r\nspam_tst = spam[-spam_idx, ]\r\n\r\n\r\n\r\nWe’ve used a somewhat small train set relative to the total size of the dataset. In practice it should likely be larger, but this is simply to keep training time low for illustration and rendering of this document.\r\n\r\n\r\nfit_caps = glm(type ~ capitalTotal, \r\n               data = spam_trn, family = binomial)\r\nfit_selected = glm(type ~ edu + money + capitalTotal + charDollar, \r\n                   data = spam_trn, family = binomial)\r\nfit_additive = glm(type ~ ., \r\n                   data = spam_trn, family = binomial)\r\nfit_over = glm(type ~ capitalTotal * (.), \r\n               data = spam_trn, family = binomial, maxit = 50)\r\n\r\n\r\n\r\nWe’ll fit four logistic regressions, each more complex than the previous. Note that we’re suppressing two warnings. The first we briefly mentioned previously.\r\n\r\n\r\n\r\nNote that, when we receive this warning, we should be highly suspicious of the parameter estimates.\r\n\r\n\r\ncoef(fit_selected)\r\n\r\n\r\n  (Intercept)           edu         money  capitalTotal    charDollar \r\n-1.1199744712 -1.9837988840  0.9784675298  0.0007757011 11.5772904667 \r\n\r\nHowever, the model can still be used to create a classifier, and we will evaluate that classifier on its own merits.\r\nWe also, “suppressed” the warning:\r\n\r\n\r\n\r\nIn reality, we didn’t actually suppress it, but instead changed maxit to 50, when fitting the model fit_over. This was enough additional iterations to allow the iteratively reweighted least squares algorithm to converge when fitting the model.\r\nEvaluating Classifiers\r\nThe metric we’ll be most interested in for evaluating the overall performance of a classifier is the misclassification rate. (Sometimes, instead accuracy is reported, which is instead the proportion of correction classifications, so both metrics serve the same purpose.)\r\n\\[\r\n\\text{Misclass}(\\hat{C}, \\text{Data}) = \\frac{1}{n}\\sum_{i = 1}^{n}I(y_i \\neq \\hat{C}({\\bf x_i}))\r\n\\]\r\n\\[\r\nI(y_i \\neq \\hat{C}({\\bf x_i})) = \r\n\\begin{cases} \r\n  0 & y_i = \\hat{C}({\\bf x_i}) \\\\\r\n  1 & y_i \\neq \\hat{C}({\\bf x_i}) \\\\\r\n\\end{cases}\r\n\\]\r\nWhen using this metric on the training data, it will have the same issues as RSS did for ordinary linear regression, that is, it will only go down.\r\n\r\n\r\n# training misclassification rate\r\nmean(ifelse(predict(fit_caps) > 0, \"spam\", \"nonspam\") != spam_trn$type)\r\n\r\n\r\n[1] 0.339\r\n\r\nmean(ifelse(predict(fit_selected) > 0, \"spam\", \"nonspam\") != spam_trn$type)\r\n\r\n\r\n[1] 0.224\r\n\r\nmean(ifelse(predict(fit_additive) > 0, \"spam\", \"nonspam\") != spam_trn$type)\r\n\r\n\r\n[1] 0.066\r\n\r\nmean(ifelse(predict(fit_over) > 0, \"spam\", \"nonspam\") != spam_trn$type)\r\n\r\n\r\n[1] 0.136\r\n\r\nBecause of this, training data isn’t useful for evaluating, as it would suggest that we should always use the largest possible model, when in reality, that model is likely overfitting. Recall, a model that is too complex will overfit. A model that is too simple will underfit. (We’re looking for something in the middle.)\r\nTo overcome this, we’ll use cross-validation as we did with ordinary linear regression, but this time we’ll cross-validate the misclassification rate. To do so, we’ll use the cv.glm() function from the boot library. It takes arguments for the data (in this case training), a model fit via glm(), and K, the number of folds. See ?cv.glm for details.\r\nPreviously, for cross-validating RMSE in ordinary linear regression, we used LOOCV. We certainly could do that here. However, with logistic regression, we no longer have the clever trick that would allow use to obtain a LOOCV metric without needing to fit the model \\(n\\) times. So instead, we’ll use 5-fold cross-validation. (5 and 10 fold are the most common in practice.) Instead of leaving a single observation out repeatedly, we’ll leave out a fifth of the data.\r\nEssentially we’ll repeat the following process 5 times:\r\nRandomly set aside a fifth of the data (each observation will only be held-out once)\r\nTrain model on remaining data\r\nEvaluate misclassification rate on held-out data\r\nThe 5-fold cross-validated misclassification rate will be the average of these misclassification rates. By only needing to refit the model 5 times, instead of \\(n\\) times, we will save a lot of computation time.\r\n\r\n\r\nlibrary(boot)\r\nset.seed(1)\r\ncv.glm(spam_trn, fit_caps, K = 5)$delta[1]\r\n\r\n\r\n[1] 0.2166961\r\n\r\ncv.glm(spam_trn, fit_selected, K = 5)$delta[1]\r\n\r\n\r\n[1] 0.1587043\r\n\r\ncv.glm(spam_trn, fit_additive, K = 5)$delta[1]\r\n\r\n\r\n[1] 0.08684467\r\n\r\ncv.glm(spam_trn, fit_over, K = 5)$delta[1]\r\n\r\n\r\n[1] 0.137\r\n\r\nNote that we’re suppressing warnings again here. (Now there would be a lot more, since were fitting a total of 20 models.)\r\nBased on these results, fit_caps and fit_selected are underfitting relative to fit_additive. Similarly, fit_over is overfitting relative to fit_additive. Thus, based on these results, we prefer the classifier created based on the logistic regression fit and stored in fit_additive.\r\nGoing forward, to evaluate and report on the efficacy of this classifier, we’ll use the test dataset. We’re going to take the position that the test data set should never be used in training, which is why we used cross-validation within the training dataset to select a model. Even though cross-validation uses hold-out sets to generate metrics, at some point all of the data is used for training.\r\nTo quickly summarize how well this classifier works, we’ll create a confusion matrix.\r\nConfusion MatrixIt further breaks down the classification errors into false positives and false negatives.\r\n\r\n\r\nmake_conf_mat = function(predicted, actual) {\r\n  table(predicted = predicted, actual = actual)\r\n}\r\n\r\n\r\n\r\nLet’s explicitly store the predicted values of our classifier on the test dataset.\r\n\r\n\r\nspam_tst_pred = ifelse(predict(fit_additive, spam_tst) > 0, \r\n                       \"spam\", \r\n                       \"nonspam\")\r\nspam_tst_pred = ifelse(predict(fit_additive, spam_tst, type = \"response\") > 0.5, \r\n                       \"spam\", \r\n                       \"nonspam\")\r\n\r\n\r\n\r\nThe previous two lines of code produce the same output, that is the same predictions, since\r\n\\[\r\n\\eta({\\bf x}) = 0 \\iff p({\\bf x}) = 0.5\r\n\\] Now we’ll use these predictions to create a confusion matrix.\r\n\r\n\r\n(conf_mat_50 = make_conf_mat(predicted = spam_tst_pred, actual = spam_tst$type))\r\n\r\n\r\n         actual\r\npredicted nonspam spam\r\n  nonspam    2057  157\r\n  spam        127 1260\r\n\r\n\\[\r\n\\text{Prev} = \\frac{\\text{P}}{\\text{Total Obs}}= \\frac{\\text{TP + FN}}{\\text{Total Obs}}\r\n\\]\r\n\r\n\r\ntable(spam_tst$type) / nrow(spam_tst)\r\n\r\n\r\n\r\n  nonspam      spam \r\n0.6064982 0.3935018 \r\n\r\nFirst, note that to be a reasonable classifier, it needs to outperform the obvious classifier of simply classifying all observations to the majority class. In this case, classifying everything as non-spam for a test misclassification rate of 0.3935018\r\nNext, we can see that using the classifier create from fit_additive, only a total of \\(137 + 161 = 298\\) from the total of 3601 email in the test set are misclassified. Overall, the accuracy in the test set it\r\n\r\n\r\nmean(spam_tst_pred == spam_tst$type)\r\n\r\n\r\n[1] 0.921133\r\n\r\nIn other words, the test misclassification is\r\n\r\n\r\nmean(spam_tst_pred != spam_tst$type)\r\n\r\n\r\n[1] 0.07886698\r\n\r\nThis seems like a decent classifier…\r\nHowever, are all errors created equal? In this case, absolutely not. The 137 non-spam emails that were marked as spam (false positives) are a problem. We can’t allow important information, say, a job offer, miss our inbox and get sent to the spam folder. On the other hand, the 161 spam email that would make it to an inbox (false negatives) are easily dealt with, just delete them.\r\nInstead of simply evaluating a classifier based on its misclassification rate (or accuracy), we’ll define two additional metrics, sensitivity and specificity. Note that these are simply two of many more metrics that can be considered. The Wikipedia page for sensitivity and specificity details a large number of metrics that can be derived form a confusion matrix.\r\nSensitivity is essentially the true positive rate. So when sensitivity is high, the number of false negatives is low.\r\n\\[\r\n\\text{Sens} = \\text{True Positive Rate} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP + FN}}\r\n\\]\r\nHere we have an R function to calculate the sensitivity based on the confusion matrix. Note that this function is good for illustrative purposes, but is easily broken. (Think about what happens if there are no “positives” predicted.)\r\n\r\n\r\nget_sens = function(conf_mat) {\r\n  conf_mat[2, 2] / sum(conf_mat[, 2])\r\n}\r\n\r\n\r\n\r\nSpecificity is essentially the true negative rate. So when specificity is high, the number of false positives is low.\r\n\\[\r\n\\text{Spec} = \\text{True Negative Rate} = \\frac{\\text{TN}}{\\text{N}} = \\frac{\\text{TN}}{\\text{TN + FP}}\r\n\\]\r\n\r\n\r\nget_spec =  function(conf_mat) {\r\n  conf_mat[1, 1] / sum(conf_mat[, 1])\r\n}\r\n\r\n\r\n\r\nWe calculate both based on the confusion matrix we had created for our classifier.\r\n\r\n\r\nget_sens(conf_mat_50)\r\n\r\n\r\n[1] 0.8892025\r\n\r\nget_spec(conf_mat_50)\r\n\r\n\r\n[1] 0.9418498\r\n\r\nRecall that we had created this classifier using a probability of \\(0.5\\) as a “cutoff” for how observations should be classified. Now we’ll modify this cutoff. We’ll see that by modifying the cutoff, \\(c\\), we can improve sensitivity or specificity at the expense of the overall accuracy (misclassification rate).\r\n\\[\r\n\\hat{C}(\\bf x) = \r\n\\begin{cases} \r\n      1 & \\hat{p}({\\bf x}) > c \\\\\r\n      0 & \\hat{p}({\\bf x}) \\leq c \r\n\\end{cases}\r\n\\]\r\nAdditionally, if we change the cutoff to improve sensitivity, we’ll decrease specificity, and vice versa.\r\nFirst let’s see what happens when we lower the cutoff from \\(0.5\\) to \\(0.1\\) to create a new classifier, and thus new predictions.\r\n\r\n\r\nspam_tst_pred_10 = ifelse(predict(fit_additive, spam_tst, type = \"response\") > 0.1, \r\n                          \"spam\", \r\n                          \"nonspam\")\r\n\r\n\r\n\r\nThis is essentially decreasing the threshold for an email to be labeled as spam, so far more emails will be labeled as spam. We see that in the following confusion matrix.\r\n\r\n\r\n(conf_mat_10 = make_conf_mat(predicted = spam_tst_pred_10, actual = spam_tst$type))\r\n\r\n\r\n         actual\r\npredicted nonspam spam\r\n  nonspam    1583   29\r\n  spam        601 1388\r\n\r\nUnfortunately, while this does greatly reduce false negatives, false positives have almost quadrupled. We see this reflected in the sensitivity and specificity.\r\n\r\n\r\nget_sens(conf_mat_10)\r\n\r\n\r\n[1] 0.9795342\r\n\r\nget_spec(conf_mat_10)\r\n\r\n\r\n[1] 0.7248168\r\n\r\nThis classifier, using \\(0.1\\) instead of \\(0.5\\) has a higher sensitivity, but a much lower specificity. Clearly, we should have moved the cutoff in the other direction. Let’s try \\(0.9\\).\r\n\r\n\r\nspam_tst_pred_90 = ifelse(predict(fit_additive, spam_tst, type = \"response\") > 0.9, \r\n                          \"spam\", \r\n                          \"nonspam\")\r\n\r\n\r\n\r\nThis is essentially increasing the threshold for an email to be labeled as spam, so far fewer emails will be labeled as spam. Again, we see that in the following confusion matrix.\r\n\r\n\r\n(conf_mat_90 = make_conf_mat(predicted = spam_tst_pred_90, actual = spam_tst$type))\r\n\r\n\r\n         actual\r\npredicted nonspam spam\r\n  nonspam    2136  537\r\n  spam         48  880\r\n\r\nThis is the result we’re looking for. We have far fewer false positives. While sensitivity is greatly reduced, specificity has gone up.\r\n\r\n\r\nget_sens(conf_mat_90)\r\n\r\n\r\n[1] 0.6210303\r\n\r\nget_spec(conf_mat_90)\r\n\r\n\r\n[1] 0.978022\r\n\r\nWhile this is far fewer false positives, is it acceptable though? Still probably not. Also, don’t forget, this would actually be a terrible spam detector today since this is based on data from a very different era of the internet, for a very specific set of people. Spam has changed a lot since 90s! (Ironically, machine learning is probably partially to blame.)\r\nThis chapter has provided a rather quick introduction to classification, and thus, machine learning. For a more complete coverage of machine learning, An Introduction to Statistical Learning is a highly recommended resource. Additionally, R for Statistical Learning has been written as a supplement which provides additional detail on how to perform these methods using R. The classification and logistic regression chapters might be useful.\r\nWe should note that the code to perform classification using logistic regression is presented in a way that illustrates the concepts to the reader. In practice, you may to prefer to use a more general machine learning pipeline such as caret in R. This will streamline processes for creating predictions and generating evaluation metrics.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-10-25T22:41:41-07:00"
    }
  ],
  "collections": []
}
